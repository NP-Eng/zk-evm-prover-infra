# Type 1 Prover Configuration
worker:
  # The zero-bin worker image to use.
  image: leovct/zero-bin-worker:develop
  # Worker pods can only be scheduled on high memory nodes.
  nodeSelector:
    key: cloud.google.com/gke-nodepool
    value: highmem-pool
  # The minimum number of workers to which the HPA can scale down.
  minWorkerCount: 1
  # The maximum number of workers to which the HPA can scale up.
  # We start with 2 nodes so we only allow 8 workers at the same time (4 per node).
  maxWorkerCount: 8

  resources:
    # The kube-scheduler will use this information to decide which node to place the Pod on.
    requests:
      # We set the memory request to 300Mi so that we can have at least 4 workers on a node of 1.4Ti.
      memory: "300Mi"
      cpu: "250m" # TODO: Update this value
    # The kubelet will enforce those limits so that the container is not allowed to use more of that resource than the limit set.
    limits:
      # We set the memory limit to 1Ti. A worker should not need more memory normally.
      memory: "1Ti"
      cpu: "4" # TODO: Update this value

# RabbitMQ Configuration
rabbitmq:
  # Cluster Configuration
  cluster:
    # The RabbitMQ image to use.
    image: rabbitmq:3.13.3
    # The number of nodes in the RabbitMQ cluster.
    # It is highly advised to use an odd number of nodes (https://www.rabbitmq.com/docs/clustering#node-count).
    nodeCount: 1
    # RabbitMQ pods can only be scheduled on standard nodes.
    nodeSelector:
      key: cloud.google.com/gke-nodepool
      value: default-pool
    # The default username.
    username: guest
    # The default password.
    password: guest

  # Horizontal Pod Autoscaler Configuration
  hpa:
    # The name of the queue to monitor.
    queue: hello
    # The interval (in seconds) at which KEDA will check the queue length and scale the deployment accordingly.
    pollingInterval: 10
